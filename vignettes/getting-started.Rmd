---
title: "Getting Started with sparkhaven"
author: "Daniel Emaasit"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# sparkhaven: Read SAS, SPSS, & STATA data files into Spark DataFrames

## What is sparkhaven?

sparkhaven is an extension for sparklyr to read SAS, SPSS, & STATA data files into Spark DataFrames. It uses different Spark packages to load such datasets in parallel in Spark.

**Currently, there's functionality for SAS only. SPSS & STATA will come shortly. Submit a pull request if interested in contributing**.

## Installation
sparkhaven requires the sparklyr package to run

### Install sparklyr
I recommend the latest stable version of sparklyr available on CRAN
```{r eval = FALSE}
install.packages("sparklyr")
```

### Install sparkhaven
Install the development version of sparkhaven from this Github repo using devtools
```{r eval = FALSE}
library(devtools)
devtools::install_github("emaasit/sparkhaven")
```

## Connecting to Spark

If Spark is not already installed, use the following sparklyr command to install your preferred version of Spark:
```{r eval = FALSE}
library(sparklyr)
spark_install(version = "2.0.0")
```

The call to `r library(sparkhaven)` will make the sparkhaven functions available on the R search path and will also ensure that the dependencies required by the package are included when we connect to Spark.
```{r eval = FALSE}
library(sparkhaven) 
```

We can create a Spark connection as follows:
```{r eval = FALSE}
sc <- spark_connect(master = "local")
```

## Reading SAS files

sparkhaven provides the function `spark_read_sas` to read SAS data files in .sas7bdat format into Spark DataFrames. It uses a Spark package called spark-sas7bdat. Here's an example.

In the example below, we read a sas data file called mtcars.sas7bdat into a table called sas_table in Spark.
```{r eval = FALSE}
mtcars_file <- system.file("extdata", "mtcars.sas7bdat", package = "sparkhaven")

mtcars_df <- spark_read_sas(sc, path = mtcars_file, table = "sas_example")
mtcars_df
```

The resulting pointer to a Spark table can be further used in dplyr statements.
```{r eval = FALSE}
library(dplyr)
mtcars_df %>% group_by(cyl) %>%
  summarise(count = n(), avg.mpg = mean(mpg), avg.displacment = mean(disp), avg.horsepower = mean(hp))
```

## Reading SPSS files
**Coming soon!**

## Reading STATA files
**Coming soon!**

## Logs & Disconnect

Look at the Spark log from R:
```{r eval = FALSE}
spark_log(sc, n = 100)
```

Now we disconnect from Spark:
```{r eval = FALSE}
spark_disconnect(sc)
```

## Acknowledgements
Thanks to RStudio for the sparklyr packages that provides functionality to create Extensions.
